1. 프로세스와 스레드의 차이점은 무엇인가요?

   프로세스는 실행 중인 프로그램의 인스턴스로, 운영체제로부터 독립적인 자원(메모리 공간, 파일 디스크립터, 레지스터 상태 등)을 할당받아 실행됩니다. 각 프로세스는 고유의 주소 공간을 가지며, 서로 직접 접근할 수 없어 보안성과 안정성이 높지만, 프로세스 간 통신(IPC)은 비용이 큽니다. 반면 스레드는 하나의 프로세스 내에서 실행되는 작업 단위로, 같은 주소 공간과 자원(코드, 데이터, 힙)을 공유하지만 스택은 개별적으로 유지합니다. 스레드는 생성 및 전환 비용이 낮고, 데이터 공유가 간편해 병렬 처리가 효율적입니다. 그러나 하나의 스레드에서 오류가 발생하면 전체 프로세스에 영향을 줄 수 있는 단점도 있습니다. 크롬 브라우저는 각 탭을 프로세스로 관리하여 하나가 중단돼도 다른 탭은 유지되지만, 게임 엔진은 내부적으로 스레드를 활용하여 렌더링, 물리 계산, 입력 등을 병렬 처리합니다. 이처럼 둘의 차이는 자원 격리 정도와 관리 단위, 응용 목적에서 뚜렷이 갈립니다.

2. 멀티스레딩의 장단점은 무엇인가요?

   멀티스레딩은 하나의 프로세스 내에서 여러 스레드를 생성해 작업을 병렬로 수행하는 방식입니다. CPU가 멀티코어일 경우 병렬 처리 성능을 극대화할 수 있으며, I/O 대기 시간이 많은 프로그램(예: 웹 서버, 채팅 프로그램 등)의 응답성을 크게 향상시킬 수 있습니다. 또한 스레드는 메모리 공간을 공유하므로 자원 사용이 효율적이고, 스레드 생성 및 컨텍스트 전환 비용이 프로세스보다 훨씬 낮습니다. 그러나 이러한 공유 특성은 동기화 문제를 야기할 수 있으며, 임계 구역을 잘못 다루면 데드락, 레이스 컨디션, 기아(Starvation) 등이 발생할 수 있습니다. 게다가 스레드 간 디버깅은 복잡하며, 잘못된 설계는 전체 시스템 불안정으로 이어질 수 있습니다. 따라서 멀티스레딩은 병렬성 향상과 자원 효율성을 제공하는 반면, 높은 설계 복잡성과 동기화 문제를 유발할 수 있어 세심한 구현이 필요합니다. 대표적인 예로 Java의 ExecutorService, C++의 std::thread가 있으며, 병렬성과 반응성을 중시하는 현대 애플리케이션에서 광범위하게 사용됩니다.

3. 컨텍스트 스위칭이란 무엇이며, 언제 발생하나요?

   컨텍스트 스위칭(Context Switching)은 CPU가 현재 실행 중인 프로세스나 스레드의 상태(레지스터, 프로그램 카운터, 스택 포인터 등)를 저장하고, 다른 프로세스나 스레드의 상태를 복원하여 실행하는 작업입니다. 이는 멀티태스킹을 가능하게 하는 핵심 메커니즘으로, 선점형 스케줄링(Priority Scheduling, Round Robin 등)에서 특히 자주 발생합니다. 현재 작업이 I/O를 기다리는 경우, 더 높은 우선순위 작업이 준비된 경우, 타임 슬라이스가 종료된 경우 등에 컨텍스트 스위칭이 발생합니다. 이 과정은 운영체제가 관리하며, PCB(Process Control Block)나 TCB(Thread Control Block)를 사용해 상태를 저장합니다. 컨텍스트 스위칭은 불가피하지만, 자주 발생하면 시스템 성능 저하를 초래합니다. 이유는 사용자 프로세스의 실행 시간이 아닌, 커널이 수행하는 전환 작업(스위칭 오버헤드)에 CPU 자원이 소모되기 때문입니다. 따라서 현대 운영체제는 최소한의 스위칭으로 최대 성능을 끌어내도록 스케줄링 알고리즘을 최적화합니다. 가령 리눅스의 CFS(Completely Fair Scheduler)는 이 점을 고려하여 설계되었습니다.

4. 데드락(Deadlock)이란 무엇이며, 발생 조건과 해결 방법은 무엇인가요?

   데드락은 여러 프로세스 또는 스레드가 서로 자원을 점유한 채, 상대방의 자원을 기다리며 영원히 대기 상태에 빠지는 상황을 의미합니다. 이는 운영체제에서 자원 할당 및 동기화 오류로 인해 발생할 수 있으며, 시스템 전체의 응답 불능으로 이어질 수 있는 심각한 문제입니다. 데드락이 발생하려면 네 가지 조건이 동시에 충족되어야 합니다: (1) 상호 배제(Mutual Exclusion), (2) 점유와 대기(Hold and Wait), (3) 비선점(No Preemption), (4) 순환 대기(Circular Wait). 이 중 하나라도 제거하면 데드락은 발생하지 않습니다. 해결 방법으로는 예방(조건 중 일부를 사전에 제거), 회피(자원 상태를 사전에 예측하여 위험 상태 진입 방지, 예: 은행원 알고리즘), 발견 및 복구(데드락 탐지 후 프로세스 종료 또는 자원 회수) 전략이 있습니다. 실시간 시스템에서는 예방이, 서버 시스템에서는 회피나 탐지 방식이 사용됩니다. 데이터베이스 트랜잭션에서는 타임아웃 기반 회피 정책이 흔히 활용됩니다. 또한, 자바의 synchronized 블록이나 POSIX 세마포어를 사용할 때 데드락을 피하려면 자원 획득 순서를 고정하거나, 타임아웃을 설정하는 것이 중요합니다.

5. 가상 메모리란 무엇이며, 동작 원리를 설명해 주세요.

   가상 메모리는 실제 물리 메모리보다 큰 논리적 메모리 공간을 프로세스에 제공하는 메모리 관리 기법입니다. 이를 통해 각 프로세스는 독립적이고 연속적인 메모리 공간을 할당받는 것처럼 동작하며, 물리 메모리의 제약 없이 실행될 수 있습니다. 이는 메모리 보호, 주소 공간 분리, 효율적인 자원 활용 등 다양한 이점을 제공합니다. 가상 메모리의 핵심은 주소 변환에 있으며, CPU에서 발생한 가상 주소는 MMU(Memory Management Unit)를 통해 페이지 단위로 물리 주소로 매핑됩니다. 운영체제는 페이지 테이블을 사용하여 이러한 변환을 관리하며, 접근하려는 페이지가 물리 메모리에 없다면 페이지 폴트(Page Fault)가 발생해 디스크에서 해당 페이지를 로드합니다. 이 과정에서 사용되는 정책에는 페이지 교체 알고리즘(LRU, FIFO 등)이 있으며, 스왑 공간은 가상 메모리 확장을 위한 디스크 영역입니다. 가상 메모리는 또한 메모리 공유(예: 공유 라이브러리)나 Copy-on-Write 기법을 통해 효율성을 극대화합니다. 이처럼 가상 메모리는 프로세스의 메모리 독립성과 시스템 자원 효율을 동시에 달성하기 위한 핵심 기술입니다.

6. 페이징(Paging)과 세그멘테이션(Segmentation)의 차이점은 무엇인가요?

   페이징과 세그멘테이션은 메모리 관리를 위한 기법으로, 가상 주소를 물리 주소로 변환하기 위해 사용됩니다. 페이징은 메모리를 고정된 크기의 페이지(Page)로 나누고, 세그멘테이션은 논리적으로 의미 있는 단위인 세그먼트(Segment)로 나눕니다. 페이징의 주요 목적은 외부 단편화(External Fragmentation)를 해결하는 것이며, 고정 크기의 페이지 때문에 프로세스가 연속된 물리 메모리를 요구하지 않습니다. 반면 세그멘테이션은 프로그램의 구조(코드, 데이터, 스택 등)를 반영하여 메모리를 논리적으로 분할하며, 각 세그먼트는 서로 다른 크기를 가집니다. 페이징은 내부 단편화(Internal Fragmentation)를 유발할 수 있고, 세그멘테이션은 외부 단편화의 위험이 있습니다. 또한 페이징은 페이지 테이블을, 세그멘테이션은 세그먼트 테이블을 사용하며 주소 변환 방식이 다릅니다. 현대 운영체제는 이 두 기법을 결합한 세그먼트-페이징 방식도 사용하며, 가상 주소를 세그먼트 → 페이지 → 물리 주소로 변환합니다. 이는 보호 기능과 메모리 관리를 동시에 강화할 수 있습니다.

7. 캐시 메모리란 무엇이며, 캐시 미스가 발생하는 원인은 무엇인가요?

   캐시 메모리는 CPU와 메인 메모리(RAM) 간의 속도 차이를 줄이기 위한 고속의 임시 저장소입니다. CPU가 자주 접근하는 데이터를 미리 저장해 두고 빠르게 접근할 수 있도록 하여 성능을 향상시킵니다. 캐시는 일반적으로 L1, L2, L3 계층 구조로 구성되며, L1이 가장 빠르지만 용량이 작고, L3는 상대적으로 느리지만 용량이 큽니다. 캐시 미스(Cache Miss)는 CPU가 요청한 데이터가 캐시에 존재하지 않아 하위 계층이나 메인 메모리에서 데이터를 다시 불러와야 할 때 발생합니다. 미스의 유형에는 세 가지가 있습니다. 첫째, 컴펄서리 미스(Compulsory Miss): 최초 접근 시 발생하며 예측 불가입니다. 둘째, 용량 미스(Capacity Miss): 캐시 크기보다 큰 데이터셋을 처리할 때 발생합니다. 셋째, 충돌 미스(Conflict Miss): 직접 매핑 방식에서 서로 다른 주소가 같은 캐시 라인을 공유할 때 발생합니다. 이러한 문제를 해결하기 위해 연관 캐시(Set-Associative Cache), 교체 알고리즘(LRU, LFU 등)이 사용됩니다. 효율적인 캐시 설계는 전체 시스템 성능에 지대한 영향을 미치며, 이는 CPU 설계의 핵심 과제 중 하나입니다.

8. LRU, LFU 등 캐시 교체 알고리즘에 대해 설명해 주세요.

   캐시 메모리는 한정된 공간을 가지므로, 새로운 데이터를 저장할 때 기존 데이터를 어떤 기준으로 제거할지 결정하는 것이 중요합니다. 이를 위해 다양한 캐시 교체 알고리즘(Cache Replacement Policy)이 존재합니다. 가장 널리 알려진 방식이 LRU(Least Recently Used)입니다. 이는 가장 오랫동안 사용되지 않은 데이터를 제거하는 방식으로, 시간 기반 접근 패턴을 반영합니다. 구현은 스택, 큐 또는 해시맵+링크드리스트 조합으로 할 수 있습니다. 그러나 구현 비용이 크고, 고정된 참조 패턴이 있을 때는 성능이 저하될 수 있습니다. LFU(Least Frequently Used)는 사용 빈도가 가장 낮은 데이터를 제거하는 방식입니다. 이는 장기적으로 자주 사용되는 데이터를 유지할 수 있다는 장점이 있지만, 오래 전에 한 번만 사용된 데이터가 계속 남아 있는 문제(기아 문제)를 유발할 수 있습니다. 이를 개선한 알고리즘으로는 Adaptive Replacement Cache (ARC)나 CLOCK 알고리즘 등이 있습니다. 현대 운영체제와 데이터베이스는 상황에 따라 알고리즘을 조합하거나 동적으로 선택합니다. 예를 들어, Linux의 페이지 캐시는 LRU 기반의 변형 알고리즘을 사용하며, 자주 쓰는 데이터와 자주 읽는 데이터를 구분하여 다르게 처리합니다.

9. 인터럽트(Interrupt)란 무엇이며, 처리 과정은 어떻게 되나요?

   인터럽트는 현재 실행 중인 작업을 잠시 중단하고, 급하게 처리해야 할 사건이 발생했음을 CPU에 알리는 신호입니다. 이는 하드웨어 또는 소프트웨어 이벤트에 의해 발생하며, CPU가 외부 장치(I/O, 타이머, 네트워크 등)나 내부 요청(예: 시스템 콜, 예외 등)에 반응할 수 있도록 돕습니다. 인터럽트가 발생하면 CPU는 현재 작업의 상태를 저장(문맥 저장)하고, 인터럽트 벡터 테이블을 참조하여 해당 인터럽트 핸들러(ISR: Interrupt Service Routine)를 실행합니다. 처리 완료 후, 저장된 상태를 복원하여 원래 작업으로 복귀합니다. 인터럽트는 우선순위를 가지며, 마스킹(masking)을 통해 특정 인터럽트를 일시적으로 무시할 수도 있습니다. 하드웨어 인터럽트는 외부 장치가 발생시키는 것이고, 소프트웨어 인터럽트는 예외나 시스템 콜 등 프로그램 내부에서 발생합니다. 인터럽트를 통해 CPU는 폴링 없이 효율적으로 장치와 상호작용할 수 있으며, 실시간 처리 시스템에서는 빠른 반응성과 시스템 자원의 효율적 사용을 가능하게 합니다. 대표적인 예로 키보드를 누를 때 OS가 이를 감지하고 입력 버퍼로 전송하는 것도 인터럽트 기반 처리입니다.

10. 시스템 콜(System Call)이란 무엇이며, 예시를 들어 설명해 주세요.

    시스템 콜(System Call)은 사용자 프로그램이 운영체제의 커널 기능(하드웨어 제어, 메모리 관리, 파일 시스템 등)을 요청할 때 사용하는 인터페이스입니다. 사용자 공간에서는 직접 하드웨어를 제어할 수 없으므로, 커널 모드 전환을 위해 시스템 콜을 사용합니다. 이는 API가 아니며, 보통 라이브러리를 통해 간접 호출됩니다. 예를 들어 C언어의 printf()는 내부적으로 write() 시스템 콜을 호출합니다. 주요 시스템 콜에는 파일 입출력(open, read, write, close), 프로세스 제어(fork, exec, wait), 메모리 관리(mmap, brk), 네트워크(socket, bind, listen) 등이 있습니다. 시스템 콜은 소프트웨어 인터럽트(INT 0x80, syscall 명령 등)를 통해 커널로 진입하며, 커널은 매개변수를 검사한 후 적절한 기능을 수행하고 결과를 사용자 공간에 반환합니다. 보안과 안정성을 위해 매개변수 유효성 검사 및 권한 체크가 수반되며, 이로 인해 일반 함수보다 오버헤드가 큽니다. 최근에는 성능 향상을 위해 Fast System Call 기술(sysenter, syscall 등)이 도입되고 있으며, 이는 현대 OS 설계에서 매우 중요한 요소입니다.

11. 커널 공간과 유저 공간의 차이점은 무엇인가요?

    운영체제는 안정성과 보안을 위해 전체 메모리 공간을 두 가지 영역으로 나눕니다: 커널 공간(Kernel Space)과 유저 공간(User Space)입니다. 커널 공간은 OS의 핵심 기능(프로세스 관리, 메모리 관리, 장치 제어 등)이 실행되는 보호된 영역이며, 이 공간에 접근하려면 특수한 권한이 필요합니다. 반면 유저 공간은 일반 애플리케이션이 실행되는 영역으로, 직접 하드웨어 접근이나 커널 코드 실행이 불가능합니다. 이런 분리는 시스템 오류가 사용자 코드에 의해 커널 전체로 확산되는 것을 방지하며, 메모리 보호(MMU로 구현됨)를 통해 사용자 프로세스 간의 접근도 차단됩니다. 프로세스가 커널 기능을 사용하고자 할 경우 시스템 콜을 통해 커널 공간으로 전환되어 필요한 처리를 요청하고, 완료되면 다시 유저 공간으로 복귀합니다. 이 전환 과정에는 오버헤드가 존재하지만, 안정성과 보안을 위한 필수 구조입니다. 커널은 일반적으로 supervisor mode(또는 ring 0)에서 실행되며, 유저 공간은 user mode(ring 3)에서 실행됩니다. 현대 OS에서는 이 공간 구분을 기반으로 스레드 컨텍스트 관리, 페이지 테이블 설계, 시스템 콜 인터페이스 구조가 설계됩니다.

12. 운영체제에서 스케줄러(Scheduler)의 역할과 주요 스케줄링 알고리즘은 무엇인가요?

    스케줄러는 운영체제의 핵심 구성 요소로, CPU 및 기타 자원에 대한 접근을 조정하는 역할을 합니다. 수많은 프로세스가 동시에 실행을 요청할 때, 어떤 프로세스를 언제, 얼마나 CPU에 할당할지 결정하는 정책을 스케줄링이라 하며, 이를 실제로 수행하는 것이 스케줄러입니다. 스케줄링은 크게 비선점형(Non-preemptive)과 선점형(Preemptive)으로 나뉘며, 전자는 프로세스가 스스로 CPU를 반환할 때까지 기다리는 방식이고, 후자는 타이머나 인터럽트로 강제로 CPU를 회수합니다. 주요 알고리즘에는 FCFS(First Come First Served), SJF(Shortest Job First), Round Robin, Priority Scheduling, Multilevel Queue, 그리고 최근 운영체제에서 널리 사용되는 CFS(Completely Fair Scheduler) 등이 있습니다. 예를 들어, 리눅스는 CFS를 사용하며, 가중치 있는 시간 공유 방식을 적용해 모든 태스크가 공정하게 CPU 시간을 받도록 설계됩니다. 스케줄러의 성능은 CPU 활용률, 처리량, 평균 대기 시간, 반응 시간 등 다양한 지표로 평가되며, 실시간 시스템에서는 마감 시간 보장을 위한 RM(Rate Monotonic), EDF(Earliest Deadline First) 같은 알고리즘도 활용됩니다.

13. 세마포어(Semaphore)와 뮤텍스(Mutex)의 차이점은 무엇인가요?

    세마포어와 뮤텍스는 모두 임계 구역(Critical Section)의 동기화를 위해 사용되는 동기화 객체입니다. 공통적으로 상호 배제를 보장하지만, 개념과 사용 방식에는 차이가 있습니다. 세마포어(Semaphore)는 카운팅 동기화 기법으로, 정수값을 기반으로 한 신호 체계입니다. 일반적으로 wait(P) 연산은 자원 요청, signal(V) 연산은 자원 반환을 의미하며, n개의 자원을 동시에 관리할 수 있습니다(예: DB 커넥션 풀). 반면, 뮤텍스(Mutex)는 이진 세마포어와 유사하지만, 오직 하나의 스레드만 접근할 수 있는 락 구조로 구성되며, 락을 획득한 스레드만 해제할 수 있습니다. 즉, 뮤텍스는 소유 개념이 존재하며 소유자가 아닌 스레드가 해제할 수 없습니다. 이는 세마포어에 비해 구조적으로 단순하고 오류 발생 가능성이 적습니다. 따라서 뮤텍스는 상호 배제에 최적화되어 있고, 세마포어는 동시성 제어나 이벤트 기반 제어에 더 적합합니다. 예를 들어 POSIX에서는 pthread_mutex_t로 뮤텍스를, sem_t로 세마포어를 제공합니다. 멀티스레드 프로그래밍에서 이 두 도구의 적절한 활용은 데드락 방지와 성능 최적화에 핵심적입니다.

14. 스택과 힙 메모리의 차이점은 무엇인가요?

    스택(Stack)과 힙(Heap)은 모두 런타임 메모리에서 프로그램 실행 중 동적으로 할당되는 영역이지만, 할당 방식과 사용 목적에 있어 큰 차이를 가집니다. 스택 메모리는 함수 호출 시 생성되는 지역 변수와 함수 매개변수, 반환 주소 등이 저장되며, 후입선출(LIFO) 방식으로 동작합니다. 컴파일 타임에 크기가 결정되고, 매우 빠른 속도로 메모리에 접근할 수 있습니다. 함수 호출이 끝나면 자동으로 메모리가 회수되므로 메모리 누수 위험이 낮습니다. 반면 힙 메모리는 malloc, new, alloca 등의 명시적인 명령어를 통해 동적으로 할당되며, 크기와 생명 주기를 프로그래머가 직접 관리해야 합니다. 때문에 할당 해제를 소홀히 하면 메모리 누수가 발생할 수 있습니다. 스택은 일반적으로 작고 빠르며, 힙은 유연하지만 느리고 복잡한 GC나 수동 해제를 필요로 합니다. 또한, 힙은 단편화(특히 외부 단편화)의 위험도 존재합니다. 현대 언어에서는 GC(Garbage Collector)를 통해 힙 관리를 자동화하며, 하드웨어 측면에서도 스택은 고정 주소 방향(보통 하향), 힙은 반대 방향으로 자라도록 설계됩니다.

15. CPU 파이프라이닝(Pipelining)이란 무엇인가요?

    CPU 파이프라이닝은 하나의 명령어를 여러 단계로 분할하고, 각 단계를 동시에 병렬로 수행하여 명령어 처리 속도를 높이는 기술입니다. 이는 산업 공정의 컨베이어 벨트와 유사하며, 명령어가 완료될 때까지 기다리지 않고 다음 명령어를 미리 처리함으로써 Instruction Throughput을 향상시킵니다. 기본적으로 파이프라인은 IF(Fetch), ID(Decode), EX(Execute), MEM(Memory Access), WB(Write Back)의 다섯 단계로 구성됩니다. 이상적으로는 n단계 파이프라인에서 한 사이클마다 한 명령어가 완료됩니다. 그러나 실제 구현에서는 데이터 종속성(데이터 해저드), 분기 예측 실패(제어 해저드), 자원 충돌(구조적 해저드) 등으로 인해 파이프라인의 효율이 저하될 수 있습니다. 이를 해결하기 위해 파이프라인 스톨, 포워딩, 브랜치 프레딕션, 슈퍼스칼라 구조 등의 기술이 사용됩니다. 예를 들어 인텔의 Out-of-Order Execution은 파이프라인 병목을 해소하기 위한 고급 기술입니다. 파이프라이닝은 고성능 CPU 설계의 핵심으로, 클럭 속도만큼이나 중요한 성능 요소입니다.

16. CPU 캐시의 계층 구조(L1, L2, L3)에 대해 설명해 주세요.

    CPU 캐시는 CPU와 메인 메모리 간의 속도 차이를 줄이기 위해 설계된 고속 메모리입니다. 일반적으로 캐시는 계층적 구조를 가지며, L1, L2, L3 캐시로 구성됩니다.

- L1 캐시는 CPU 코어에 가장 가까운 위치에 있으며, 명령어 캐시와 데이터 캐시로 분리되어 있는 경우가 많습니다. 용량은 작지만 접근 속도가 매우 빠릅니다(수 ns 이내).

- L2 캐시는 L1보다 크고 느리며, 코어별로 독립되거나 공유될 수 있습니다.

- L3 캐시는 CPU의 모든 코어가 공유하는 형태로 존재하며, L2보다 크지만 접근 속도는 더 느립니다.

  이 계층 구조는 지역성(Locality) 원칙에 기반해 설계되었으며, 자주 사용되는 데이터를 상위 캐시에 위치시켜 CPU 접근 지연을 최소화합니다. 계층이 많을수록 캐시 히트율을 높일 수 있지만, 동기화 및 설계가 복잡해지고 캐시 미스 시 오버헤드도 커집니다. 따라서 각 계층의 크기, 속도, 접근 정책, 교체 알고리즘(LRU, LFU 등)은 프로세서 설계의 중요한 요소입니다. 예시로 인텔 i7 CPU는 32KB L1, 256KB L2, 최대 12MB L3 캐시를 가집니다.

17. 하드웨어 인터럽트와 소프트웨어 인터럽트의 차이점은 무엇인가요?

    하드웨어 인터럽트(Hardware Interrupt)와 소프트웨어 인터럽트(Software Interrupt)는 둘 다 현재 실행 중인 작업을 중단하고 특정 처리를 요청하는 메커니즘이지만, 발생 원인과 목적, 처리 방식에 차이가 있습니다.

    - 하드웨어 인터럽트는 외부 장치(예: 키보드 입력, 디스크 I/O, 네트워크 패킷 등)에서 발생합니다. CPU는 인터럽트 라인을 통해 신호를 수신하며, 이는 비동기적으로 발생하고, 인터럽트 컨트롤러(PIC/APIC)가 이를 관리해 적절한 핸들러로 연결합니다. 이는 실시간성 및 반응성 확보에 매우 중요합니다.
    - 소프트웨어 인터럽트는 프로그램 코드 내부에서 발생합니다. 예를 들어, 사용자 공간에서 커널 기능을 요청할 때 시스템 콜을 통해 소프트웨어 인터럽트를 발생시킵니다. x86 아키텍처에서는 INT 0x80, SYSCALL 명령 등이 사용됩니다.

      하드웨어 인터럽트는 비예측적으로 발생하므로 빠른 우선순위 처리와 컨텍스트 스위칭이 필수이며, 소프트웨어 인터럽트는 예측 가능한 흐름의 일부로 설계됩니다. 이처럼 하드웨어 인터럽트는 장치 제어, 소프트웨어 인터럽트는 커널 진입 및 예외 처리를 위한 수단으로 각각 활용됩니다.

18. 컴퓨터가 부팅되는 과정을 설명해 주세요.

    컴퓨터 부팅은 전원이 켜진 순간부터 운영체제가 완전히 실행되어 사용자와 상호작용 가능한 상태가 될 때까지의 일련의 초기화 과정입니다. 단계별로 다음과 같은 과정을 거칩니다:

    1. 전원 공급 및 하드웨어 초기화
       전원이 켜지면 CPU는 가장 먼저 ROM에 내장된 BIOS(Basic Input/Output System) 또는 UEFI(Unified Extensible Firmware Interface) 코드를 실행합니다. 이 코드는 POST(Power-On Self Test)를 통해 메모리, CPU, 디스크 등의 기본 장치 이상 유무를 점검합니다.

    2. 부트 디바이스 탐색 및 부트로더 로딩
       BIOS/UEFI는 저장장치에서 MBR(Master Boot Record) 또는 EFI 파티션을 찾아, 부트로더(예: GRUB, LILO)를 메모리에 로드합니다.

    3. 운영체제 커널 로딩
       부트로더는 운영체제의 커널을 메모리로 적재하고 제어를 넘깁니다. 이때 커널은 하드웨어 정보를 수집하고, 드라이버를 초기화하며, 루트 파일시스템을 마운트합니다.

    4. init/systemd 실행 및 사용자 환경 구성
       커널이 완전히 초기화되면 PID 1번 프로세스인 init 또는 systemd가 실행되고, 로그인 서비스, GUI 환경, 데몬 등을 순차적으로 시작합니다.

    --=[이 과정을 통해 사용자는 셸이나 GUI로 진입하게 되며, 이는 전형적인 하드웨어 → 펌웨어 → 커널 → 사용자 공간으로의 흐름을 나타냅니다.

19. 바이너리와 라이브러리의 차이점은 무엇인가요?
    바이너리(Binary)와 라이브러리(Library)는 컴파일된 결과물이지만, 그 역할과 사용 방식은 명확히 구분됩니다.

- 바이너리는 컴파일된 실행파일로, 사용자가 직접 실행 가능한 독립 프로그램입니다. 예를 들어 리눅스에서 /bin/ls, 윈도우에서 notepad.exe 등이 이에 해당합니다. 바이너리는 기계어 코드로 구성되어 있으며, 운영체제의 로더(Loader)에 의해 메모리에 적재되고 실행됩니다.

- 라이브러리는 코드 재사용을 목적으로 만들어진 컴포넌트로, 다른 프로그램에서 호출되는 함수나 클래스들을 포함합니다. 정적 라이브러리(.a, .lib)는 컴파일 시 바이너리에 포함되고, 동적 라이브러리(.so, .dll)는 실행 시 연결됩니다.

  예를 들어 C 프로그램에서 printf()를 호출하면, 이는 실제로 glibc라는 라이브러리에 정의되어 있는 함수입니다. 이때 링커는 해당 심볼을 연결하거나, 실행 시 동적으로 로딩합니다.

  즉, 바이너리는 독립 실행 목적, 라이브러리는 기능 제공 목적입니다. 이 차이는 프로그램 구조와 빌드/배포 방식에도 중요한 영향을 미칩니다.

20. 컴파일러와 인터프리터의 차이점은 무엇인가요?

    컴파일러(Compiler)와 인터프리터(Interpreter)는 고급 프로그래밍 언어로 작성된 소스 코드를 기계가 이해할 수 있는 형태로 변환하는 도구이지만, 작동 방식과 실행 방식에서 뚜렷한 차이를 보입니다.

    - 컴파일러는 전체 소스 코드를 한 번에 분석하여 기계어(Binary)로 번역하고, 독립된 실행파일을 생성합니다. 이 방식은 실행 전 오류 검출이 가능하고, 실행 속도가 빠릅니다. C, C++ 등의 언어가 대표적이며, gcc, clang이 대표적인 컴파일러입니다.
    - 인터프리터는 소스 코드를 한 줄씩 읽고 즉시 실행합니다. 실행 전 전체 코드를 분석하지 않으므로 실행은 느리지만, 개발 및 디버깅은 용이합니다. 파이썬, 루비, 자바스크립트 등이 대표적이며, Python 인터프리터는 REPL 기반 개발 환경에서도 사용됩니다
    - 단일 언어가 둘 다 사용하는 경우도 있습니다. 예를 들어 Java는 .java 소스를 컴파일해 .class 바이트코드로 만든 후, JVM이 이를 인터프리트하거나 JIT 컴파일하여 실행합니다. 요약하면, 컴파일러는 사전 번역, 인터프리터는 실시간 실행 방식입니다.
